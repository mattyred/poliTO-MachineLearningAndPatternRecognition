{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d1456a-ac6d-4b3b-bc9a-8d6207285ac7",
   "metadata": {},
   "source": [
    "#Â Lab 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "285d9238-449b-4692-8cda-0ed13e9011ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96491a36-0cb4-436e-b6f5-9c88da589c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    lInf = []\n",
    "    f=open('data/inferno.txt', encoding=\"ISO-8859-1\")\n",
    "    for line in f:\n",
    "        lInf.append(line.strip())\n",
    "    f.close()\n",
    "    lPur = []\n",
    "    f=open('data/purgatorio.txt', encoding=\"ISO-8859-1\")\n",
    "    for line in f:\n",
    "        lPur.append(line.strip())\n",
    "    f.close()\n",
    "    lPar = []\n",
    "    f=open('data/paradiso.txt', encoding=\"ISO-8859-1\")\n",
    "    for line in f:\n",
    "        lPar.append(line.strip())\n",
    "    f.close() \n",
    "    return lInf, lPur, lPar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1370dc-f5f0-495f-9563-71555fc93ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(l, n):\n",
    "    lTrain, lTest = [], []\n",
    "    for i in range(len(l)):\n",
    "        if i % n == 0:\n",
    "            lTest.append(l[i])\n",
    "        else:\n",
    "            lTrain.append(l[i])          \n",
    "    return lTrain, lTest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5306130-cb2c-4f0e-a25d-eabda9e5ff02",
   "metadata": {},
   "source": [
    "First, the data for training and testing must be retrieved. For example, lInf_train is the training set for cantica Inferno and lInf_evaluation is the evaluation set for the model we will train. The same for the other two cantiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96c7f2c-83c7-4c01-859a-4431906618e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lInf, lPur, lPar = load_data()\n",
    "lInf_train, lInf_evaluation = split_data(lInf, 4)\n",
    "lPur_train, lPur_evaluation = split_data(lPur, 4)\n",
    "lPar_train, lPar_evaluation = split_data(lPar, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6743c-b709-4148-a01e-d6a5f7c28478",
   "metadata": {},
   "source": [
    "Now we obtain a set (sDictCommon) containing all the possible words of each cantica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10788004-cefe-4916-bc0b-dc995a2375a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(lTercets):\n",
    "    sDict = set([])\n",
    "    for s in lTercets:\n",
    "        words = s.split()\n",
    "        for w in words:\n",
    "            sDict.add(w)\n",
    "    return sDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "425566ad-28b2-45c9-b09f-ad8a1c806d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "hCls2Idx = {'inferno': 0, 'purgatorio': 1, 'paradiso': 2}\n",
    "\n",
    "hlTercetsTrain = {\n",
    "    'inferno': lInf_train,\n",
    "    'purgatorio': lPur_train,\n",
    "    'paradiso': lPar_train\n",
    "}\n",
    "sDictCommon = set([]) # set of all the words that can be found in the cantica\n",
    "for cls in hlTercetsTrain: # Loop over class labels (the three cantica)\n",
    "    lTercets = hlTercetsTrain[cls]\n",
    "    sDictCls = build_dictionary(lTercets)\n",
    "    sDictCommon = sDictCommon.union(sDictCls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce37d5-845a-4567-b480-36effa580c09",
   "metadata": {},
   "source": [
    "Using a pseudo-count strategy to avoid having words with count 0 in one of the cantiche (for example a word appearing in Inferno could never appear in Paradiso, to avoid that the word in Paradiso will have count=0 we initialize each counter with eps, that is a hyperparameter of our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7edeec0-3da5-482c-942d-fd959260487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_clsLogProb = {}\n",
    "eps = 0.001\n",
    "for cls in hlTercetsTrain: # Loop over class labels\n",
    "    h_clsLogProb[cls] = {w: eps for w in sDictCommon} # Create a dictionary for each class that contains all words as keys and the pseudo-count as initial values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729d46bc-21e5-4ef2-86c6-eaa8f1f43338",
   "metadata": {},
   "source": [
    "The dictionary h_clsLogProb contains three keys (Inf, Pur, Par) and for each key the whole list of words appearing in all the commedia with the counter initialized to 0.001.\n",
    "Now we compute the actual word-count separately for each cantica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fdf433b-9b13-4a3d-9aba-5231efc3c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in hlTercetsTrain: # Loop over class labels\n",
    "    tercets_class = hlTercetsTrain[cls]\n",
    "    for tercet in tercets_class: # Loop over all tercets of the class\n",
    "        words = tercet.split()\n",
    "        for w in words: # Loop over words of the given tercet\n",
    "            h_clsLogProb[cls][w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015b724-1913-4792-a90b-ee3a17c0d8ea",
   "metadata": {},
   "source": [
    "Now h_clsLogProb will contain for each key (cantica) how many times each word appears in the cantica itself. All the words that still have as counter value eps means that they never appear in that cantica but they appear in at least one of the other two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f50b5-bb3d-417b-8ac4-fa4ef38178e2",
   "metadata": {},
   "source": [
    "Now we compute the logarithm of the frequency of each word for each class: log(N_{cls,w} / N_{cls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a3f4d6-a647-4994-b01b-cde864d33219",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in hlTercetsTrain: # Loop over class labels\n",
    "    nWordsCls = sum(h_clsLogProb[cls].values()) # Get all occurrencies of words in cls and sum them. this is the number of words (including pseudo-counts)\n",
    "    for w in h_clsLogProb[cls]: # Loop over all words\n",
    "        h_clsLogProb[cls][w] = np.log(h_clsLogProb[cls][w]) - np.log(nWordsCls) # Compute log N_{cls,w} / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c96b3-e2c7-439e-bf7b-a3eec53c839c",
   "metadata": {},
   "source": [
    "Now it's time to compute the matrix of class-conditional log-likelihoods for each class each tercet in lTercets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9e0503b-b45c-49b7-948f-8fc6316fbe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def S1_compute_logLikelihoods(h_clsLogProb, text):\n",
    "    logLikelihoodCls = {cls: 0 for cls in h_clsLogProb}\n",
    "    for cls in h_clsLogProb: # Loop over classes\n",
    "        for word in text.split(): # Loop over words\n",
    "            if word in h_clsLogProb[cls]:\n",
    "                logLikelihoodCls[cls] += h_clsLogProb[cls][word]\n",
    "    return logLikelihoodCls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "057e917b-2502-4400-8478-602d6360b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hCls2Idx = {cls:idx for idx, cls in enumerate(sorted(h_clsLogProb))} \n",
    "# This is a map between textual labels (keys of h_clsLogProb) and matrix rows. \n",
    "# If not provided, automatic mapping based on alphabetical oreder is used\n",
    "# Inferno: 0\n",
    "# Paradiso: 1\n",
    "# Purgatorio: 2\n",
    "\n",
    "S = np.zeros((len(h_clsLogProb), len(lTercets)))\n",
    "for tIdx, tercet in enumerate(lTercets):\n",
    "    hScores = S1_compute_logLikelihoods(h_clsLogProb, tercet)\n",
    "    for cls in h_clsLogProb: # We sort the class labels so that rows are ordered according to alphabetical order of labels\n",
    "        clsIdx = hCls2Idx[cls]\n",
    "        S[clsIdx, tIdx] = hScores[cls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45140e9-c4b4-485d-b9b7-a6a996e18572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
