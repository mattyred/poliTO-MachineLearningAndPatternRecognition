{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772fcb37-e3cf-4c68-b5e4-910371cb7c3c",
   "metadata": {},
   "source": [
    "## MVG Gaussian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a58a59b-8bf7-4276-be31-2feb7a1f3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "def load_iris():\n",
    "    D, L = sklearn.datasets.load_iris()['data'].T, sklearn.datasets.load_iris()['target']\n",
    "    return D, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c680977-27d6-43a6-85f7-d48c1dc50523",
   "metadata": {},
   "outputs": [],
   "source": [
    "D, L = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca7062a5-5849-4a78-b2cd-6626b17abc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_db_2to1(D, L, seed=0):\n",
    "    nTrain = int(D.shape[1]*2.0/3.0) # 2/3 of the dataset D are used for training, 1/3 for validation\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1]) # take a random array of 150 elements, each element is 0<x<=149 (np.arange(150))\n",
    "    idxTrain = idx[0:nTrain] # first 100 are indices of training samples \n",
    "    idxTest = idx[nTrain:] # remaining 50 are indices of validation samples\n",
    "    DTR = D[:, idxTrain] # D for training\n",
    "    DTE = D[:, idxTest] # D for validation\n",
    "    LTR = L[idxTrain] # L for training\n",
    "    LTE = L[idxTest] # L for validation\n",
    "    return (DTR, LTR), (DTE, LTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "349a74a4-c95e-45ff-888e-c327f840f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DTR, LTR), (DTE, LTE) = split_db_2to1(D, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b19ab44-ee0b-469a-9ecd-43969ead6a03",
   "metadata": {},
   "source": [
    "Now we have to compute the ML solution. First we compute the empirical mean and variance for each class label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea202c-eb1d-43d0-88b5-cfd0720167ce",
   "metadata": {},
   "source": [
    "The training phase consists in computing the empirical class mean and the empirical class covariance matrix given the training samples DTR. Here we're fitting a normal distribution to our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00e412b3-a61b-4de2-8eac-08fec3e88305",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_classes = [] # list of empiracal mean for each class\n",
    "cov_classes = [] # list of covariance matrix for each class\n",
    "for i in set(LTR):\n",
    "    DTR_class_i = DTR[:,LTR==i]\n",
    "    N_class_i = DTR_class_i.shape[1]\n",
    "    mu_class_i = DTR_class_i.mean(axis=1).reshape(-1,1)\n",
    "    cov_class_i = 1/N_class_i * np.dot(DTR_class_i-mu_class_i, (DTR_class_i-mu_class_i).T)\n",
    "    mu_classes.append(mu_class_i)\n",
    "    cov_classes.append(cov_class_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94a7d69-9358-41ed-af34-06f162884b4f",
   "metadata": {},
   "source": [
    "The test phase consists in computing the normal density for each testing sample, thus the probability for each test sample to belong to either class 0 or 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db69955d-d7d7-4d48-9272-5e09fe533f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logpdf_GAU_ND_1sample(x,mu,C):\n",
    "    M = x.shape[0] # num of features of sample x\n",
    "    mu = mu.reshape(M,1) # mean of the sample\n",
    "    xc = x - mu # x centered\n",
    "    invC = np.linalg.inv(C)\n",
    "    _,log_abs_detC = np.linalg.slogdet(C)\n",
    "    return -M/2 * np.log(2*np.pi) - 1/2 * log_abs_detC - 1/2 * np.dot(np.dot(xc.T,invC),xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f3a5722-204a-4022-ad49-f7cc7eb9e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros(shape=(3,DTE.shape[1]))\n",
    "for i in range(DTE.shape[1]):\n",
    "    xt = DTE[:,i:i+1] # test sample xt\n",
    "    # now compute the probability density related to each class label for the sample xt\n",
    "    score = np.zeros(shape=(3,1))\n",
    "    for j in set(LTE):\n",
    "        mu = mu_classes[j]\n",
    "        C = cov_classes[j]\n",
    "        score[j,:] = np.exp(logpdf_GAU_ND_1sample(xt,mu,C))\n",
    "    S[:,i:i+1] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4e4efac-58b0-4e66-8070-eae105e15885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.661338147750939e-15\n"
     ]
    }
   ],
   "source": [
    "SJoint = 1/3 * S # assuming that the prior probability is 1/3 for each class\n",
    "SJoint_sol = np.load('Solution/SJoint_MVG.npy')\n",
    "print(np.abs(SJoint_sol - SJoint).max()) # test if it's correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6028d989-97ba-4b2c-adf0-d1adca57e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMarginal = SJoint.sum(0).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2d285d7-c604-4eb1-950c-b4d4aff44a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPost = np.zeros((3,50))\n",
    "for c in range(3):\n",
    "    SJoint_c = SJoint[c,:].reshape(-1,1)\n",
    "    SPost_c = (SJoint_c / SMarginal).reshape(1,-1)\n",
    "    SPost[c,:] = SPost_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4c90792-b9ba-47f3-abd1-47859fa2ec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate 4.000000%\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = np.argmax(SPost,axis=0)\n",
    "corrected_assigned_labels = LTE==predicted_labels\n",
    "acc = sum(corrected_assigned_labels) / len(LTE)\n",
    "err = 1-acc\n",
    "print('Error rate %f%%' % (err * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba3ff99-58e1-4672-ae2b-3a1af8c2e386",
   "metadata": {},
   "source": [
    "Using logarithms calculus to avoid numerical issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfa630dc-a05e-42be-af39-eec31520c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logSJoint = np.log(SJoint) + np.log(1/3)\n",
    "logSMarginal = scipy.special.logsumexp(logSJoint, axis=0).reshape(1,-1)\n",
    "log_SPost = logSJoint - logSMarginal  \n",
    "SPost_ = np.exp(log_SPost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363a42e-1538-470b-953d-723c9d5396f6",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd819a40-d18c-4149-8c3e-f2ea17faaf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate 4.000000%\n"
     ]
    }
   ],
   "source": [
    "cov_classes_nbayes = []\n",
    "for i in range(3):\n",
    "    cov_classes_nbayes.append(cov_classes[i]*np.identity(4))\n",
    "SPost_nbayes = compute_post_probabilities(DTE,LTE,mu_classes,cov_classes_nbayes)\n",
    "predicted_labels = np.argmax(SPost_nbayes,axis=0)\n",
    "corrected_assigned_labels = LTE==predicted_labels\n",
    "acc = sum(corrected_assigned_labels) / len(LTE)\n",
    "err = 1-acc\n",
    "print('Error rate %f%%' % (err * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d6df9bae-4dd8-451d-9c84-7dcfa4cf59ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_post_probabilities(DTE, LTE, mu_classes, cov_classes):\n",
    "    num_classes = len(set(LTE))\n",
    "    num_test_samples = DTE.shape[1]\n",
    "    S = np.zeros(shape=(num_classes,num_test_samples))\n",
    "    for i in range(num_test_samples):\n",
    "        xt = DTE[:,i:i+1] # test sample xt\n",
    "        # now compute the probability density related to each class label for the sample xt\n",
    "        score = np.zeros(shape=(num_classes,1))\n",
    "        for j in set(LTE):\n",
    "            mu = mu_classes[j]\n",
    "            C = cov_classes[j]\n",
    "            score[j,:] = np.exp(logpdf_GAU_ND_1sample(xt,mu,C))\n",
    "        S[:,i:i+1] = score\n",
    "        \n",
    "    prior_prob = 1 / num_test_samples\n",
    "    SJoint = S * prior_prob\n",
    "    SMarginal = SJoint.sum(0).reshape(-1,1)\n",
    "    # compute class posterior probabilities SPost = SJoint / SMarginal\n",
    "    SPost = np.zeros((num_classes,num_test_samples))\n",
    "    for c in range(num_classes):\n",
    "        SJoint_c = SJoint[c,:].reshape(-1,1)\n",
    "        SPost_c = (SJoint_c / SMarginal).reshape(1,-1)\n",
    "        SPost[c,:] = SPost_c\n",
    "    return SPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "19af58ba-b081-47a5-a819-e260d70f59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(SPost ,LTE):\n",
    "    predicted_labels = np.argmax(SPost,axis=0)\n",
    "    corrected_assigned_labels = LTE==predicted_labels\n",
    "    acc = sum(corrected_assigned_labels) / len(LTE)\n",
    "    err = 1-acc\n",
    "    return (predicted_labels, acc, err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee671a-29d4-40d1-b6f7-024aa67ece2a",
   "metadata": {},
   "source": [
    "## Tied Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "921bc966-7106-4db9-8779-60a547e005c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = [sum(LTR == i) for i in range(3)]\n",
    "tied_cov = 0\n",
    "for i in range(3):\n",
    "    tied_cov += (num_samples_per_class[i] * cov_classes[i])\n",
    "tied_cov *= 1/sum(num_samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "478d6b3a-2d67-4d90-bc68-92ad8f68e2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23637589, 0.09525344, 0.1364944 , 0.03614529],\n",
       "       [0.09525344, 0.11618517, 0.05768855, 0.0357726 ],\n",
       "       [0.1364944 , 0.05768855, 0.14992811, 0.03746458],\n",
       "       [0.03614529, 0.0357726 , 0.03746458, 0.04291763]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tied_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba653f18-26f9-4a84-ab2d-1bb1877eb964",
   "metadata": {},
   "source": [
    "To compute the probabilities maybe we can use the compute_post_probabilities function but inside we have only one covariance matrix so the row C = cov_classes[j] should be removed?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
